Dataset $\mathcal{D}={x_i, x_i}_{n=1}^N$
**Regression Goal:** predict target $y_*$ from new input $x_*$
**Amount of input features**: How many dimensions our y's have. 

We can do this prediction by fitting a line through our model based on our data, so we can predict $y_*$ with $f(x_*)$
$$
f_\textbf{w}(x) = w_0 + w_1x + w_3x^2 + \dots + w_Mx^M
$$
$w_j$ are the **parameters** of this **model** $f(x)$

Questions arise:
- What degree should we pick for the polynomial (model structure)
- How do we choose the weights (model parameters)

## Fitting model params
For each training point measure the squared error
$$
e^2_i = (y_i-f(x_i))^2
$$
find the parameters that minimise the sum of these errors across datapoints
$$
\mathcal{E}(\textbf{w}) = \sum_N e^2_i
$$
$f_\textbf{w}(x)$ is a function of the param vector $\textbf{w} =[w_0, w_1, \dots, w_M]^\top$

### Least squares notation
$\vec{y} = [y_1,\dots, y_N]^\top$: vector of training targets (real outputs of $x$)
$\vec{f} = [f_w(x_1), \dots, f_w(x_N)]$ : vector of our predicted outputs using our model
$\vec{e} = \vec{y} - \vec{f}$: vector of prediction errors

Sum is therefore given by
$$
E(\vec{w}) = \|\vec{e}\|^2 = \vec{e}^\top \vec{e} = (\vec{y} - \vec{f})^\top (\vec{y} - \vec{f})
$$
$\phi_j(x) = x^j$ is a **basis function** of our **linear in the parameters** model.
$$f_w(x) = w_0 \cdot 1 + w_1 x + w_2 x^2 + \dots + w_M x^M = \sum_{j=0}^M w_j \phi_j(x)$$
$\Phi_{ij} = \phi_j(x_i)$ allows us to write $\mathbf{f} = \mathbf{\Phi} \mathbf{w}$.

### A Gradient View
The sum of squared errors is a convex function of $\mathbf{w}$:
$$E(\mathbf{w}) = (\mathbf{y} - \mathbf{f})^\top (\mathbf{y} - \mathbf{f}) = (\mathbf{y} - \mathbf{\Phi} \mathbf{w})^\top (\mathbf{y} - \mathbf{\Phi} \mathbf{w})$$
The gradient with respect to the weights is:
$$\frac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = -2\mathbf{\Phi}^\top (\mathbf{y} - \mathbf{\Phi} \mathbf{w}) = 2\mathbf{\Phi}^\top \mathbf{\Phi} \mathbf{w} - 2\mathbf{\Phi}^\top \mathbf{y}$$
The weight vector $\hat{\mathbf{w}}$ that sets the gradient to zero minimises $E(\mathbf{w})$:
$$\hat{\mathbf{w}} = (\mathbf{\Phi}^\top \mathbf{\Phi})^{-1} \mathbf{\Phi}^\top \mathbf{y}$$
### A Geometrical View
This is the matrix form of the **Normal equations**:
- The vector of training targets $\mathbf{y}$ lives in an N-dimensional vector space.
- The vector of training predictions $\mathbf{f}$ lives in the same space, but it is constrained to being generated by the $M + 1$ columns of matrix $\mathbf{\Phi}$.
- The error vector $\mathbf{e}$ is minimal if it is orthogonal to all columns of $\mathbf{\Phi}$:
$$\mathbf{\Phi}^\top \mathbf{e} = 0 \iff \mathbf{\Phi}^\top (\mathbf{y} - \mathbf{\Phi} \mathbf{w}) = 0$$


