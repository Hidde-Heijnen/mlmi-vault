Normally there is a lot of noise in our data, so we can't expect to be able to put a single line through all our points, otherwise we'll be overfitting. 

In reality you can assume that the data comes from a function + noise
$$
y_n = f_w(x_n)+ \epsilon_n
$$
- We can generalise the noise to have a mean $\mu$ and variance $\sigma^2$. For a general Gaussian density function, $\epsilon_n \sim \mathcal{N}(\epsilon_n;\mu, \sigma^2)$:
$$
p(\epsilon_n) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( - \frac{(\epsilon_n - \mu)^2}{2\sigma^2} \right)
$$
## Vector and matrix notation view of the noise
- $\mathbf{e} = [\epsilon_1, \dots, \epsilon_N]^\top$ stacks the independent noise terms:  $$\mathbf{e} \sim \mathcal{N}(\mathbf{e}; 0, \sigma_{\text{noise}}^2 \mathbf{I})$$
$$p(\mathbf{e}) = \prod_{n=1}^N p(\epsilon_n) = \left( \frac{1}{\sqrt{2\pi \sigma_{\text{noise}}^2}} \right)^N \exp \left( - \frac{\mathbf{e}^\top \mathbf{e}}{2\sigma_{\text{noise}}^2} \right)$$
- Given that $\mathbf{y} = \mathbf{f} + \mathbf{e}$, we can write the probability of $\mathbf{y}$ given $\mathbf{f}$:
$$p(\mathbf{y}|\mathbf{f}, \sigma_{\text{noise}}^2) = \mathcal{N}(\mathbf{y}; \mathbf{f}, \sigma_{\text{noise}}^2) = \left( \frac{1}{\sqrt{2\pi \sigma_{\text{noise}}^2}} \right)^N \exp \left( - \frac{\|\mathbf{y} - \mathbf{f}\|^2}{2\sigma_{\text{noise}}^2} \right)$$$$= \left( \frac{1}{\sqrt{2\pi \sigma_{\text{noise}}^2}} \right)^N \exp \left( - \frac{\mathbf{E(w)}}{2\sigma_{\text{noise}}^2} \right)$$ - $\mathbf{E(w)} = \sum_{n=1}^N (y_n - f_w(x_n))^2 = \|\mathbf{y} - \mathbf{\Phi w}\|^2 = \mathbf{e}^\top \mathbf{e}$ is the sum of squared errors. - Since $\mathbf{f} = \mathbf{\Phi w}$, we can write $p(\mathbf{y}|\mathbf{w}, \sigma_{\text{noise}}^2) = p(\mathbf{y}|\mathbf{f}, \sigma_{\text{noise}}^2)$ for a given $\mathbf{\Phi}$.

## Likelihood function
--- 
The **likelihood** of the parameters is the probability of the data given the parameters:
- $p(\mathbf{y}|\mathbf{w}, \sigma_{\text{noise}}^2)$ is the probability of the observed data given the weights.
- $\mathcal{L}(\mathbf{w}) \propto p(\mathbf{y}|\mathbf{w}, \sigma_{\text{noise}}^2)$ is the likelihood of the weights.

### Maximum likelihood:
- We can fit the model weights to the data by maximising the likelihood:

$$
\hat{\mathbf{w}} = \arg\max \mathcal{L}(\mathbf{w}) = \arg\max \exp \left( -\frac{\mathbf{E(w)}}{2\sigma_{\text{noise}}^2} \right) = \arg\min \mathbf{E(w)}
$$

- With an additive Gaussian independent noise model, the **maximum likelihood** and the **least squares** solutions are the same.
- But... we still have not solved the prediction problem! We still overfit.

## Multiple explanations of the data
---
- We do not **believe** all models are equally probable to explain the data.
- We may **believe** a simpler model is more probable than a complex one.
#### Model complexity and uncertainty:
- We do not **know** what particular function generated the data.
- More than one of our models can perfectly fit the data.
- We **believe** more than one of our models could have generated the data.
- We want to reason in terms of a **set of possible explanations**, not just one.


